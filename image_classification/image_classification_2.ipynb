{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       im_name  label\n",
      "0  00016cd.jpg      6\n",
      "1  0001808.jpg      2\n",
      "2  0002399.jpg      1\n",
      "3  0003973.jpg      3\n",
      "4  00061cc.jpg      4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "train_csv = 'data/train.csv'\n",
    "test_csv = 'data/test.csv'\n",
    "train_images_path = 'data/train_ims/'\n",
    "test_images_path = 'data/test_ims/'\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "print(train_df.head())\n",
    "\n",
    "missing_files = [f for f in train_df['im_name'] if not os.path.exists(os.path.join(train_images_path, f))]\n",
    "if missing_files:\n",
    "    print(\"Missing files:\", missing_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "\n",
    "def load_and_preprocess_images(df, folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        image_path = f\"{folder}/{row['im_name']}\"\n",
    "        image = imread(image_path)\n",
    "        images.append(image)\n",
    "        labels.append(row['label'])\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "train_data, train_labels = load_and_preprocess_images(train_df, train_images_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "\n",
    "class HOGFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, resize_shape=(64, 64), orientations=12, pixels_per_cell=(8, 8), cells_per_block=(4, 4)):\n",
    "        self.resize_shape = resize_shape\n",
    "        self.orientations = orientations\n",
    "        self.pixels_per_cell = pixels_per_cell\n",
    "        self.cells_per_block = cells_per_block\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        hog_features = []\n",
    "        for img in X:\n",
    "            if img.ndim == 3:  \n",
    "                img = rgb2gray(img)\n",
    "            if self.resize_shape:\n",
    "                img = resize(img, self.resize_shape, anti_aliasing=True)\n",
    "            # Extract HOG features\n",
    "            features = hog(\n",
    "                img,\n",
    "                orientations=self.orientations,\n",
    "                pixels_per_cell=self.pixels_per_cell,\n",
    "                cells_per_block=self.cells_per_block,\n",
    "                block_norm='L2-Hys',\n",
    "                visualize=False\n",
    "            )\n",
    "            hog_features.append(features)\n",
    "        return np.array(hog_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('hog', HOGFeatureExtractor()),  # Custom HOG feature extractor\n",
    "    ('svm', SVC(kernel='rbf', C=1.0, gamma='scale'))  # SVM classifier\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(train_data, train_labels)\n",
    "print(\"Model training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_csv)\n",
    "test_data, test_labels = load_and_preprocess_images(test_df, test_images_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = pipeline.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('hog', HOGFeatureExtractor()),  # Custom HOG feature extractor\n",
    "    ('scaler', StandardScaler()),  # Standardize the features\n",
    "    ('pca', PCA(n_components=0.95)),  # Apply PCA for dimensionality reduction\n",
    "    ('svm', SVC(kernel='rbf', C=10, gamma='auto'))  # SVM classifier\n",
    "])\n",
    "\n",
    "pipeline.fit(train_data, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = pipeline.predict(test_data)\n",
    "\n",
    "\n",
    "# Save predictions to CSV\n",
    "test_df['label'] = test_predictions\n",
    "test_df.to_csv('updated_test_predictions2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'updated_test_predictions_rf.csv'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('hog', HOGFeatureExtractor(orientations = 9, pixels_per_cell=(8,8), cells_per_block=(3,3))),  # Custom HOG feature extractor\n",
    "    ('scaler', StandardScaler()),  # Standardize the features\n",
    "    ('pca', PCA(n_components=0.99)),  # Apply PCA for dimensionality reduction\n",
    "    ('svm', SVC(kernel='rbf', C=10, gamma='auto'))  # SVM classifier\n",
    "])\n",
    "\n",
    "\n",
    "# Train the classifier\n",
    "pipeline.fit(train_data, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = pipeline.predict(test_data)\n",
    "\n",
    "test_df['label'] = test_predictions\n",
    "test_df.to_csv('updated_test_predictions_rf.csv', index=False)\n",
    "print(\"Predictions saved to 'updated_test_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page reference: 3, Frames: [3], Page Faults: 1\n",
      "Page reference: 1, Frames: [3, 1], Page Faults: 2\n",
      "Page reference: 4, Frames: [3, 1, 4], Page Faults: 3\n",
      "Page reference: 1, Frames: [3, 1, 4], Page Faults: 3\n",
      "Page reference: 5, Frames: [3, 1, 4, 5], Page Faults: 4\n",
      "Page reference: 9, Frames: [1, 4, 5, 9], Page Faults: 5\n",
      "Page reference: 2, Frames: [1, 5, 9, 2], Page Faults: 6\n",
      "Page reference: 6, Frames: [1, 9, 2, 6], Page Faults: 7\n",
      "Page reference: 5, Frames: [1, 2, 6, 5], Page Faults: 8\n",
      "Page reference: 3, Frames: [1, 6, 5, 3], Page Faults: 9\n",
      "Page reference: 5, Frames: [1, 6, 5, 3], Page Faults: 9\n",
      "Page reference: 8, Frames: [1, 5, 3, 8], Page Faults: 10\n",
      "Page reference: 9, Frames: [1, 5, 8, 9], Page Faults: 11\n",
      "Page reference: 7, Frames: [1, 5, 9, 7], Page Faults: 12\n",
      "Page reference: 9, Frames: [1, 5, 9, 7], Page Faults: 12\n",
      "Page reference: 3, Frames: [1, 5, 9, 3], Page Faults: 13\n",
      "Page reference: 2, Frames: [1, 5, 9, 2], Page Faults: 14\n",
      "Page reference: 3, Frames: [1, 5, 9, 3], Page Faults: 15\n",
      "Page reference: 8, Frames: [1, 5, 9, 8], Page Faults: 16\n",
      "Page reference: 4, Frames: [1, 5, 9, 4], Page Faults: 17\n",
      "Total Page Faults: 17\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, deque\n",
    "\n",
    "def lfu_page_replacement(page_references, num_frames):\n",
    "    frames = []\n",
    "    frequency = defaultdict(int)\n",
    "    page_faults = 0\n",
    "    page_order = deque()  # To keep track of the order of pages for tie-breaking\n",
    "\n",
    "    for page in page_references:\n",
    "        if page in frames:\n",
    "            frequency[page] += 1\n",
    "        else:\n",
    "            page_faults += 1\n",
    "            if len(frames) < num_frames:\n",
    "                frames.append(page)\n",
    "                frequency[page] = 1\n",
    "                page_order.append(page)\n",
    "            else:\n",
    "                # Find the least frequently used page\n",
    "                lfu_page = min(frames, key=lambda x: (frequency[x], page_order.index(x)))\n",
    "                frames.remove(lfu_page)\n",
    "                del frequency[lfu_page]\n",
    "                page_order.remove(lfu_page)\n",
    "                frames.append(page)\n",
    "                frequency[page] = 1\n",
    "                page_order.append(page)\n",
    "\n",
    "        print(f\"Page reference: {page}, Frames: {frames}, Page Faults: {page_faults}\")\n",
    "\n",
    "    return page_faults\n",
    "\n",
    "page_references = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9, 3, 2, 3, 8, 4]\n",
    "num_frames = 4\n",
    "total_page_faults = lfu_page_replacement(page_references, num_frames)\n",
    "print(f\"Total Page Faults: {total_page_faults}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page reference: 3, Frames: [3, -1, -1, -1], Page Faults: 1, Reference Bits: [1, 0, 0, 0]\n",
      "Page reference: 1, Frames: [3, 1, -1, -1], Page Faults: 2, Reference Bits: [1, 1, 0, 0]\n",
      "Page reference: 4, Frames: [3, 1, 4, -1], Page Faults: 3, Reference Bits: [1, 1, 1, 0]\n",
      "Page reference: 1, Frames: [3, 1, 4, -1], Page Faults: 3, Reference Bits: [1, 1, 1, 0]\n",
      "Page reference: 5, Frames: [3, 1, 4, 5], Page Faults: 4, Reference Bits: [1, 1, 1, 1]\n",
      "Page reference: 9, Frames: [9, 1, 4, 5], Page Faults: 5, Reference Bits: [1, 0, 0, 0]\n",
      "Page reference: 2, Frames: [9, 2, 4, 5], Page Faults: 6, Reference Bits: [1, 1, 0, 0]\n",
      "Page reference: 6, Frames: [9, 2, 6, 5], Page Faults: 7, Reference Bits: [1, 1, 1, 0]\n",
      "Page reference: 5, Frames: [9, 2, 6, 5], Page Faults: 7, Reference Bits: [1, 1, 1, 1]\n",
      "Page reference: 3, Frames: [9, 2, 6, 3], Page Faults: 8, Reference Bits: [0, 0, 0, 1]\n",
      "Page reference: 5, Frames: [5, 2, 6, 3], Page Faults: 9, Reference Bits: [1, 0, 0, 1]\n",
      "Page reference: 8, Frames: [5, 8, 6, 3], Page Faults: 10, Reference Bits: [1, 1, 0, 1]\n",
      "Page reference: 9, Frames: [5, 8, 9, 3], Page Faults: 11, Reference Bits: [1, 1, 1, 1]\n",
      "Page reference: 7, Frames: [5, 8, 9, 7], Page Faults: 12, Reference Bits: [0, 0, 0, 1]\n",
      "Page reference: 9, Frames: [5, 8, 9, 7], Page Faults: 12, Reference Bits: [0, 0, 1, 1]\n",
      "Page reference: 3, Frames: [3, 8, 9, 7], Page Faults: 13, Reference Bits: [1, 0, 1, 1]\n",
      "Page reference: 2, Frames: [3, 2, 9, 7], Page Faults: 14, Reference Bits: [1, 1, 1, 1]\n",
      "Page reference: 3, Frames: [3, 2, 9, 7], Page Faults: 14, Reference Bits: [1, 1, 1, 1]\n",
      "Page reference: 8, Frames: [3, 2, 8, 7], Page Faults: 15, Reference Bits: [0, 0, 1, 0]\n",
      "Page reference: 4, Frames: [3, 2, 8, 4], Page Faults: 16, Reference Bits: [0, 0, 1, 1]\n",
      "Total Page Faults: 16\n"
     ]
    }
   ],
   "source": [
    "def clock_page_replacement(page_references, num_frames):\n",
    "    frames = [-1] * num_frames\n",
    "    reference_bits = [0] * num_frames\n",
    "    pointer = 0\n",
    "    page_faults = 0\n",
    "\n",
    "    for page in page_references:\n",
    "        if page in frames:\n",
    "            reference_bits[frames.index(page)] = 1\n",
    "        else:\n",
    "            page_faults += 1\n",
    "            while reference_bits[pointer] == 1:\n",
    "                reference_bits[pointer] = 0\n",
    "                pointer = (pointer + 1) % num_frames\n",
    "            frames[pointer] = page\n",
    "            reference_bits[pointer] = 1\n",
    "            pointer = (pointer + 1) % num_frames\n",
    "\n",
    "        print(f\"Page reference: {page}, Frames: {frames}, Page Faults: {page_faults}, Reference Bits: {reference_bits}\")\n",
    "\n",
    "    return page_faults\n",
    "\n",
    "page_references = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9, 3, 2, 3, 8, 4]\n",
    "num_frames = 4\n",
    "total_page_faults = clock_page_replacement(page_references, num_frames)\n",
    "print(f\"Total Page Faults: {total_page_faults}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine how many disk blocks are required to store the block allocation table for a 64 GiB thumb-drive with a block size of 4 KiB and 32-bit disk block pointers, we need to follow these steps:\n",
    "\n",
    "Calculate the total number of disk blocks on the thumb-drive:\n",
    "\n",
    "Total storage capacity: 64 GiB\n",
    "Disk block size: 4 KiB\n",
    "$$[ \\text{Total number of disk blocks} = \\frac{\\text{Total storage capacity}}{\\text{Disk block size}} ]\n",
    "$$\n",
    "$$\n",
    "[ \\text{Total number of disk blocks} = \\frac{64 \\times 2^{30} \\text{ bytes}}{4 \\times 2^{10} \\text{ bytes}} ]\n",
    "$$\n",
    "$$\n",
    "[ \\text{Total number of disk blocks} = \\frac{64 \\times 2^{30}}{4 \\times 2^{10}} = \\frac{64 \\times 2^{20}}{4} = 16 \\times 2^{20} = 2^{24} ]\n",
    "$$\n",
    "So, the total number of disk blocks is (2^{24}).\n",
    "\n",
    "Calculate the size of the block allocation table:\n",
    "\n",
    "Each entry in the block allocation table is 32 bits (4 bytes).\n",
    "The block allocation table needs one entry for each disk block.\n",
    "$$\n",
    "[ \\text{Size of the block allocation table} = \\text{Total number of disk blocks} \\times \\text{Size of each entry} ]\n",
    "$$\n",
    "$$\n",
    "[ \\text{Size of the block allocation table} = 2^{24} \\times 4 \\text{ bytes} = 2^{24} \\times 2^2 \\text{ bytes} = 2^{26} \\text{ bytes} ]\n",
    "$$\n",
    "\n",
    "Calculate the number of disk blocks required to store the block allocation table:\n",
    "$$\n",
    "Disk block size: 4 KiB (which is (2^{12}) bytes)\n",
    "[ \\text{Number of disk blocks required} = \\frac{\\text{Size of the block allocation table}}{\\text{Disk block size}} ]\n",
    "$$\n",
    "$$\n",
    "[ \\text{Number of disk blocks required} = \\frac{2^{26} \\text{ bytes}}{2^{12} \\text{ bytes}} = 2^{14} ]\n",
    "$$\n",
    "So, the number of disk blocks required to store the block allocation table is (2^{14}).\n",
    "\n",
    "Therefore, the number of disk blocks required to store the block allocation table is (2^{14} = 16,384) blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total number of disk blocks on the thumb-drive:\n",
    "\n",
    "Total storage capacity: 64 GiB\n",
    "Disk block size: 4 KiB\n",
    "$$\n",
    "[ \\text{Total number of disk blocks} = \\frac{\\text{Total storage capacity}}{\\text{Disk block size}} ]\n",
    "$$\n",
    "$$\n",
    "[ \\text{Total number of disk blocks} = \\frac{64 \\times 2^{30} \\text{ bytes}}{4 \\times 2^{10} \\text{ bytes}} ]\n",
    "$$\n",
    "$$\n",
    "[ \\text{Total number of disk blocks} = \\frac{64 \\times 2^{30}}{4 \\times 2^{10}} = \\frac{64 \\times 2^{20}}{4} = 16 \\times 2^{20} = 2^{24} ]\n",
    "$$\n",
    "So, the total number of disk blocks is (2^{24}).\n",
    "\n",
    "Calculate the size of the data block bitmap:\n",
    "\n",
    "Each bit in the bitmap represents one disk block.\n",
    "The total number of bits required is equal to the total number of disk blocks.\n",
    "$$\n",
    "[ \\text{Size of the data block bitmap} = \\text{Total number of disk blocks} \\text{ bits} ]\n",
    "$$\n",
    "$$\n",
    "[ \\text{Size of the data block bitmap} = 2^{24} \\text{ bits} ]\n",
    "$$\n",
    "Convert the size of the data block bitmap to bytes:\n",
    "\n",
    "There are 8 bits in a byte.\n",
    "$$\n",
    "[ \\text{Size of the data block bitmap in bytes} = \\frac{\\text{Size of the data block bitmap in bits}}{8} ]\n",
    "$$\n",
    "$$\n",
    "[ \\text{Size of the data block bitmap in bytes} = \\frac{2^{24} \\text{ bits}}{8} = 2^{21} \\text{ bytes} ]\n",
    "$$\n",
    "Calculate the number of disk blocks required to store the data block bitmap:\n",
    "\n",
    "Disk block size: 4 KiB (which is (2^{12}) bytes)\n",
    "$$\n",
    "[ \\text{Number of disk blocks required} = \\frac{\\text{Size of the data block bitmap in bytes}}{\\text{Disk block size}} ]\n",
    "$$\n",
    "$$\n",
    "[ \\text{Number of disk blocks required} = \\frac{2^{21} \\text{ bytes}}{2^{12} \\text{ bytes}} = \\frac{2^{21}}{2^{12}} = 2^{9} ]\n",
    "$$\n",
    "So, the number of disk blocks required to store the data block bitmap is (2^{9}).\n",
    "\n",
    "Therefore, the number of disk blocks required to store the data block bitmap is (2^{9} = 512) blocks.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
