{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AUwbFEaMiWi"
   },
   "source": [
    "# Spam classifier with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:05:59.155299Z",
     "start_time": "2022-10-27T05:05:58.015019Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbs5g3YbuH59",
    "outputId": "a6288dcd-5dde-44c7-ab3a-b3fea4887280"
   },
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class EmailCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        no_header=True,\n",
    "        to_lowercase=True,\n",
    "        url_to_word=True,\n",
    "        num_to_word=True,\n",
    "        remove_punc=True,\n",
    "    ):\n",
    "        self.no_header = no_header\n",
    "        self.to_lowercase = to_lowercase\n",
    "        self.url_to_word = url_to_word\n",
    "        self.num_to_word = num_to_word\n",
    "        self.remove_punc = remove_punc\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        cleaned_emails = []\n",
    "        for email in X:\n",
    "            if self.no_header:\n",
    "                email = self.remove_header(email)\n",
    "            if self.to_lowercase:\n",
    "                email = email.lower()\n",
    "\n",
    "            words = email.split()\n",
    "            if self.url_to_word:\n",
    "                words = self.convert_url_to_word(words)\n",
    "            if self.num_to_word:\n",
    "                words = self.convert_num_to_word(words)\n",
    "            email = \" \".join(words)\n",
    "            if self.remove_punc:\n",
    "                email = \"\".join([c for c in email if c.isalnum() or c.isspace()])\n",
    "            cleaned_emails.append(email)\n",
    "        return cleaned_emails\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_header(email):\n",
    "        return email[email.index(\"\\n\\n\") :]\n",
    "\n",
    "    @staticmethod\n",
    "    def is_url(string):\n",
    "        return re.match(\n",
    "            \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "            string,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_url_to_word(words):\n",
    "        return [\"URL\" if EmailCleaner.is_url(word) else word for word in words]\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_num_to_word(words):\n",
    "        return [\"NUM\" if word.isdigit() else word for word in words]\n",
    "\n",
    "\n",
    "def download_and_extract(url, dataset_dir=\"data\"):\n",
    "    tar_dir = os.path.join(dataset_dir, \"tar\")\n",
    "    os.makedirs(tar_dir, exist_ok=True)\n",
    "    filename = url.rsplit(\"/\", 1)[-1]\n",
    "    tarpath = os.path.join(tar_dir, filename)\n",
    "\n",
    "    class DownloadProgressBar(tqdm):\n",
    "        def update_to(self, b=1, bsize=1, tsize=None):\n",
    "            if tsize is not None:\n",
    "                self.total = tsize\n",
    "            self.update(b * bsize - self.n)\n",
    "\n",
    "    if not os.path.exists(tarpath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        with DownloadProgressBar(\n",
    "            unit=\"B\", unit_scale=True, miniters=1, desc=url.split(\"/\")[-1]\n",
    "        ) as t:\n",
    "            urlretrieve(url, tarpath, reporthook=t.update_to)\n",
    "        print(\"\\nDownload completed.\")\n",
    "    else:\n",
    "        print(f\"{filename} already downloaded.\")\n",
    "\n",
    "    print(\"Extracting files...\")\n",
    "    with tarfile.open(tarpath) as tar:\n",
    "        dirname = os.path.join(dataset_dir, tar.getmembers()[0].name.split(\"/\")[0])\n",
    "        if os.path.isdir(dirname):\n",
    "            shutil.rmtree(dirname)\n",
    "        tar.extractall(path=dataset_dir)\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "    cmds_path = os.path.join(dirname, \"cmds\")\n",
    "    if os.path.isfile(cmds_path):\n",
    "        os.remove(cmds_path)\n",
    "    return dirname\n",
    "\n",
    "\n",
    "def load_dataset(dirpath):\n",
    "    files = []\n",
    "    filepaths = glob.glob(os.path.join(dirpath, \"*\"))\n",
    "    for path in filepaths:\n",
    "        with open(path, \"rb\") as f:\n",
    "            content = f.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "            files.append(content)\n",
    "    return files\n",
    "\n",
    "\n",
    "def download_datasets():\n",
    "    spam_url = \"https://github.com/comp3314/hw-data/releases/download/hw3/20050311_spam_2.tar.bz2\"\n",
    "    easy_ham_url = \"https://github.com/comp3314/hw-data/releases/download/hw3/20030228_easy_ham_2.tar.bz2\"\n",
    "    hard_ham_url = \"https://github.com/comp3314/hw-data/releases/download/hw3/20030228_hard_ham.tar.bz2\"\n",
    "\n",
    "    spam = load_dataset(download_and_extract(spam_url))\n",
    "    easy_ham = load_dataset(download_and_extract(easy_ham_url))\n",
    "    hard_ham = load_dataset(download_and_extract(hard_ham_url))\n",
    "\n",
    "    X = spam + easy_ham + hard_ham\n",
    "    y = np.concatenate((np.ones(len(spam)), np.zeros(len(easy_ham) + len(hard_ham))))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset download and preparation...\n",
      "20050311_spam_2.tar.bz2 already downloaded.\n",
      "Extracting files...\n",
      "Extraction completed.\n",
      "20030228_easy_ham_2.tar.bz2 already downloaded.\n",
      "Extracting files...\n",
      "Extraction completed.\n",
      "20030228_hard_ham.tar.bz2 already downloaded.\n",
      "Extracting files...\n",
      "Extraction completed.\n",
      "Dataset preparation completed.\n",
      "The number of training samples: 2436\n",
      "The number of test samples: 610\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "(2436, 108735)\n",
      "(610, 108735)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download and prepare the dataset\n",
    "print(\"Starting dataset download and preparation...\")\n",
    "X, y = download_datasets()\n",
    "print(\"Dataset preparation completed.\")\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"The number of training samples: {len(X_train)}\")\n",
    "print(f\"The number of test samples: {len(X_test)}\")\n",
    "\n",
    "# Preprocess the data\n",
    "print(\"Starting preprocessing...\")\n",
    "email_cleaner = EmailCleaner()\n",
    "count_vectorizer = CountVectorizer()\n",
    "prepare_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"email_cleaner\", email_cleaner),\n",
    "        (\"count_vectorizer\", count_vectorizer),\n",
    "    ]\n",
    ")\n",
    "X_all = X_train + X_test\n",
    "prepare_pipeline.fit(X_all)\n",
    "X_all_transformed = prepare_pipeline.transform(X_all)\n",
    "num_train = len(X_train)\n",
    "X_train = X_all_transformed[:num_train]\n",
    "X_test = X_all_transformed[num_train:]\n",
    "print(\"Preprocessing completed.\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZYuW1qnvth8"
   },
   "source": [
    "## Train spam classifiers with MLP \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:25.443785Z",
     "start_time": "2022-10-27T05:06:25.420443Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yRP2qwQAAomF",
    "outputId": "916d66dc-a2a7-4b6a-9c71-1abc1882e3c5"
   },
   "outputs": [],
   "source": [
    "# === Your code here ===\n",
    "# ======================\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "configurations = [(10,), (20,), (40,), (5, 5), (10, 10), (20, 20)]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for config in configurations:\n",
    "    model = MLPClassifier(hidden_layer_sizes=config)\n",
    "    model.fit(X_train, y_train)\n",
    "    models[config] = model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W17SOnErAomF"
   },
   "source": [
    "## Evaluating classifiers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:26.371331Z",
     "start_time": "2022-10-27T05:06:26.364811Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrXaSqBIAomF",
    "outputId": "19fd9a6b-156a-4bb1-bc0c-82cb05855199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: (10,)\n",
      "Accuracy: 0.9721311475409836\n",
      "Precision: 0.9781818181818182\n",
      "Recall: 0.9607142857142857\n",
      "\n",
      "Configuration: (20,)\n",
      "Accuracy: 0.9737704918032787\n",
      "Precision: 0.9748201438848921\n",
      "Recall: 0.9678571428571429\n",
      "\n",
      "Configuration: (40,)\n",
      "Accuracy: 0.9737704918032787\n",
      "Precision: 0.9714285714285714\n",
      "Recall: 0.9714285714285714\n",
      "\n",
      "Configuration: (5, 5)\n",
      "Accuracy: 0.9672131147540983\n",
      "Precision: 0.9814814814814815\n",
      "Recall: 0.9464285714285714\n",
      "\n",
      "Configuration: (10, 10)\n",
      "Accuracy: 0.9721311475409836\n",
      "Precision: 0.9713261648745519\n",
      "Recall: 0.9678571428571429\n",
      "\n",
      "Configuration: (20, 20)\n",
      "Accuracy: 0.9704918032786886\n",
      "Precision: 0.9781021897810219\n",
      "Recall: 0.9571428571428572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Your code here ===\n",
    "# ======================\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "for config, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Configuration: {config}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTbxtjvHohqJ"
   },
   "source": [
    "## Step 4: Ensemble of classifiers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:29.164535Z",
     "start_time": "2022-10-27T05:06:28.530051Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwS_cYldop6_",
    "outputId": "b88ccd6a-10c9-40f6-9486-3e62bc1ebfd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble classifier performance:\n",
      "Accuracy: 0.9737704918032787\n",
      "Precision: 0.9714285714285714\n",
      "Recall: 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "# === Your code here ===\n",
    "\n",
    "# ======================\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ensemble_configs = [((40,), models[(40,)]), ((5, 5), models[(5, 5)]), ((20,), models[(20,)])]\n",
    "ensemble_model = VotingClassifier(estimators=ensemble_configs)\n",
    "\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "accuracy_ensemble = accuracy_score(y_test, y_pred)\n",
    "precision_ensemble = precision_score(y_test, y_pred)\n",
    "recall_ensemble = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"Ensemble classifier performance:\")\n",
    "print(f\"Accuracy: {accuracy_ensemble}\")\n",
    "print(f\"Precision: {precision_ensemble}\")\n",
    "print(f\"Recall: {recall_ensemble}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
